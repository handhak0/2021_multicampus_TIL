# Crawling 

## 배경지식 

- 웹 서버 <--> 웹 클라이언트 / GET, POST 방식이 있다. 

- HTML5, CSS3, JavaScript  
- HTML 구조 



- 클라이언트가 서버에 접속을 요청하면 html 문서를 전달한다. 



### 웹 사이트 구축 

웹 서버를 만들고, 클라이언트를 만들고 하는 조합 

- Java - JSP, Apache Tomcat 
- Python - Django, Flask 
- NodeJS - Express 

=> 서버에서 html 코드를 만들어서 클라이언트 단에서 랜더링해서 보여주는 것 

- React (요즘은 react가 대세) 

=> html 코드 중에서 바뀐 부분만 보내줌 

 



### 웹 페이지 주소, URL

![image-20210817094303917](C:\Users\sonso\Desktop\Python\멀티캠퍼스\00.TIL\03.E-learning\md-images\image-20210817094303917.png)

- IP : host를 구분해주는 것 (IP주소)
- TCP / 포트 번호: 다양한 응용프로그램을 접속하게 해주는 것 ( 포트 ex) HTTP 80번포트임 ) - 각자 다른 포트 번호를 씀 



### HTML

class . : 클래스 이름에 스페이스가 들어간 것은 모두 스페이스를 .으로 변경해줘야 함 

id # 



### CSS 

**셀렉터**

자식 셀렉터

자손 셀렉터 





## Web Crawling

### 대상 

- 웹 상의 자원
- 정적인 문서 또는 API 같은 서비스 



### 툴, 라이브러리 

#### 라이브러리 

- Beautiful Soup : 파이썬 
- Jsoup : 자바 버전
- Selenium : 브라우저를 이용 



### BeautifulSoup

#### Local html 파일 열기 

``` python
from bs4 import BeautifulSoup
with open('<파일이름.html>') as fp : 
    # soup이라는 객체 안에 html 코드 담기
    soup = BeautifulSoup(fp, 'html.parser') # html.parser = html 분석기
```



##### find( )

```python
first_div = soup.find('div') # find는 한 개만 찾음 
first_div 
```

- find( ) : 한 개만 찾음 



##### find_all( )

```python
all_div = soup.find_all('div')
len(all_div) # div 몇 개인지 확인 
```

- find_all( ) : 모두 찾음 

```python
for div in all_divs : 
    print(div)
```

- div를 한 개씩 출력 



#### 태그와 속성 가져오기 

##### select_one( )

css selector로 하나만 찾는 메서드

```python
ex_id_div = soup.select_one('#ex_id')# id가 ex_id인 데이터 가져옴 
ex_sample_div = soup.select_one('.ex_class.sample') # class가 ex_class sample인 데이터 가져옴 
```



##### select( )

css selector로 모두를 찾는 메서드 

```python
ex_id_divs = soup.select('#ex_id') # return은 리스트로 

sample_divs = soup.select('.sample') 
```



#### 결과 가져오기 

ex) a 태그에 있는 '네이버'라는 글씨를 가져오는 것 



**태그 사이에 있는 txt 가져오기**

##### get_text( )

```python
result = soup.select_one('.a.sample').get_text()
result 
```

##### string 

```python
result = soup.select_one('.a.sample').string
result 
```



**속성 값 가져오기** 

##### ['속성']

```python
href = soup.select_one('.a.sample')['href']
href 
```



<예제> 

id = "ex_id"인 div에서 p 내용물을 가져오기 

````python
```
<div id = "ex_id"> 
	<p>X</p> <p>Y</p> <p>Z</p>
</div>
```

# all_ps = soup.select_one('#ex_id').select('p').get_text()
# 위에는 내 풀이 되는지 확인해보기 

ex_id_div = soup.select_one('#ex_id')
all_ps = ex_id_div.select('p')
for p in all_ps : 
    print(p.string)
````

 

#### Request 

외부 접속 시 사용 하는 request 

```python
import requests 
import urllib.request 
import urllib.parser
```



### 예제 : 지니 Top200 차트

- 모듈 불러오기 

```python
import requests
import pandas as pd 
```



- 지니차트 불러오기 

```python
# Genie Top 200 

url = 'http://mw.genie.co.kr/chart/top200'
req = requests.get(url) # get 방식으로 가져옴 
html = req.text # html 코드 담기 
html # 결과에서 "The security policy of the connection request is blocked." 크롤링이 막힘 
```

***해결방법***

: [header user agent 사용하기](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/User-Agent)

```python
# 헤더 추가 
header = {'User-Agent' : 
          'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'}
req = requests.get(url, headers = header)
html = req.text
html
```



- 찾으려고 하는 데이터의 태그 찾기  및 데이터 프레임 생성 

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(html, 'html.parser') 


# 한 페이지 당 50개씩 있기 때문에 페이지 4개를 추출

rank_list, title_list, artist_list, album_list  = [], [], [], []

for page in range(1,5) : 
  url = f'https://www.genie.co.kr/chart/top200?ditc=D&ymd=20210817&hh=14&rtm=Y&pg={page}' # url 페이지 번호
  header = {'User-Agent' : 
          'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'} # 나 사람이야 알려주는 것
  req = requests.get(url, headers = header) # get 방식으로 요청 
  soup = BeautifulSoup(req.text, 'html.parser') # 결과 html 담기
  trs = soup.select('tr.list') # 순위 정보 담기 테이블 리스트 

  for tr in trs : # 테이블에서 순위 정보 하나씩 읽기 
    # rank
    rank = int(tr.select_one('.number').get_text().split('\n')[0])
    
    # title
    title = tr.select_one('.info').select_one('.title').get_text().strip()
    
    # artist 
    artist = tr.select_one('.info').select_one('.artist').get_text()
    
    # album
    album = tr.select_one('.info').select_one('.albumtitle').get_text().strip()

    rank_list.append(rank)
    title_list.append(title)
    artist_list.append(artist)
    album_list.append(album)


# 데이터 프레임 생성     
df = pd.DataFrame({
    '순위' : rank_list, 
    '곡명' : title_list, 
    '가수' : artist_list, 
    '앨범' : album_list})

df.head()
```



### 예제 : 식신 양재역 검색 결과 

- 모듈 불러오기 

``` python
import requests 
import pandas as pd 
from urllib.parse import quote # 한글 데이터를 전달하기 위한 모듈 
```



- 양재역 검색 데이터 불러오기 

``` python
base_url = 'https://www.siksinhot.com/search?keywords='
url = f'{base_url}{quote("양재역")}'
req = requests.get(url)
html = req.text
html
```



- 찾으려고 하는 데이터의 태그 찾기 

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(html, 'html.parser') 
lis = soup.select('div.listTy1 > ul > li') # 맛집 리스트 12개 담기 
```



- 데이터 프레임 생성 

```python
name_li, score_li, menu_li, tel_li, addr_li = [], [], [], [], []

for li in lis : 
  href =li.select_one('a')['href'] # 양재역 검색결과에서는 원하는 데이터를 찾을 수 없음. sub url로 접근해야 함 
  sub_url = 'https://www.siksinhot.com' + href  # sub_url 생성 
  req = requests.get(sub_url) 
  sub_soup = BeautifulSoup(req.text, 'html.parser')
  # 가게 정보 
  store = sub_soup.select_one('.title')
  info = store.find('h3').get_text()
  # 별점 
  score = store.select_one('h3 > strong').string
  try : 
    info.find(score) # info에 별점이 없는 경우가 있음 
  except : 
    info = store.select_one('.store_name_score').get_text()
    score = '평가중' # info에 별점이 없는 경우 
  # 가게
  name = info[:info.find(score)]
  # 메뉴 
  menu = sub_soup.select('.store_info p')[1].get_text()
  # 전화번호 
  tel = sub_soup.select_one('.p_tel p').get_text()
  # 주소
  addr = sub_soup.select_one('.txt_adr').get_text()

  name_li.append(name)
  score_li.append(score)
  menu_li.append(menu)
  tel_li.append(tel)
  addr_li.append(addr)


df = pd.DataFrame({
    '가게' : name_li,
    '별점' : score_li, 
    '메뉴' : menu_li,
    '전화번호' : tel_li,
    '주소' : addr_li
    })
df.head()
```





